---
title: "Spatio-Temporal Data Analysis Project"
date: "`r Sys.Date()`"
output:
   pdf_document:
     fig_caption: true
     number_sections: true
fontsize: 12pt
---


![](./foscari.jpg)

# Patterns in foreign sims connected to OpenWiFi-Milan  {-}

Author: Bernardi Riccardo - 864018

Professor: Isadora Antoniano-Villalobos

\newpage
\tableofcontents
\newpage
<!-- \listoffigures -->
<!-- \newpage -->
<!-- \listoftables -->
<!-- \newpage -->

```{r include = FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=5, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)

rm(list=ls())
```

# Statement of the Problem

The project is assigned to every student with a different topic. the topic have to be different between students and should be original to achieve a better score. Students can work in couple and this obviously involves a proportional workload, "more people, more work to be done". The project should include the analysis of spatial data or temporal data, also both together is possible but the complexity involved is very high. The project should include an introduction, a description of the data, motivation of the choice, a detailed analysis with all the possible tools(interesting tools should be explained and can improve the overall mark).

I Worked by my own, the whole code and report is developed only by me. The code is not plotted on the resulting pdf but is available on the source. I used best practices to write good code such as KISS(keeping the code very very simple) and commenting the code. I used a lot of libraries preferring guaranteed code instead writing unrealiable code. I tried to use realiable libraries since R packages can be developed by all the people and can contain errors. I choosed a very very original problem, this can be checked by simply inputting the name of this project on the internet, no previous works were done until the time i'm writing(or at least they do not appear to me on t hegoogle page).

# Introduction & Motivation

The dataset that I've chosen is about the presence of foreign smartphone's sims to the OpenWifi of the Municipality of Milan. This data is open and available on the website data.gov.it. The reasons why I would like to go further with this project is that I strongly believe that are present seasonalities that can be interesting to be analysed but also can be more interesting to relate the outliers to some events that happened in the past with a certain mediatic relevance. In practice I would like to both analyse trend and seasonalities to know in which months there are more foreign people and if the trend is increasing in time and both search for outlier peaks to be related to important happenings in the Milan city. Finally I would like to forecast the possible presences in the new year in the city of Milan.


# Data Description

The dataset comes from the open data provided by all the municipalities of Milan. This repository is available at dati.gov.it. From this repository I selected the data going from June of 2017 to October of the 2019.

Characteristics of the DataSet:

- the dataset contains 2 columns "Date, Number_of_Foreign_Sims"
- has 868 rows
- Dates goes from from 05/06/17 to 30/10/19 (~2 years)
- the datasets have no NA
- no lacking days
- the "Number_of_Foreign_Sims" is a discrete variable about total number of foreign sims in a certain Date connected to the OpenWifi of Milan


```{r}
setwd("~/Documents/GitHub/STDA-project-proposal")
set.seed(25061997)

require(zoo)
require(xts)
data <- read.csv("opendatamilano2017.csv",sep = ";")
data <- rbind(data, read.csv("opendatamilano2018.csv",sep = ";"))
data <- rbind(data, read.csv("opendatamilano2019.csv",sep = ";"))

data$prefix <- NULL
data$country <- NULL
data$num <- NULL
data$total.ita.sim <- NULL

ll <- aggregate.data.frame(data$total.foreign.sim,by=list(data$date),FUN=mean)
names(ll)[2] <- "total.foreign.sim"
names(ll)[1] <- "Date"
data <- ll
```

# Exploration of the Data

```{r}
print("minimum, lower-hinge, median, upper-hinge, maximum)")
fivenum(data$total.foreign.sim)
```

```{r}
hist(data$total.foreign.sim)
```


## Preprocessing

Checking Nans

```{r}
sum(is.na(data$Date))
sum(is.na(data$total.foreign.sim))
```

Checking limit values

```{r}
min(data$total.foreign.sim)
max(data$total.foreign.sim)
```

```{r}
mean(data$total.foreign.sim)
mean.data <- mean(data$total.foreign.sim)
```

```{r}
sd(data$total.foreign.sim)
sd.data <- sd(data$total.foreign.sim)
```

Elements that are good in our ts stand between mean$\pm$std

```{r}
up.m.sd <- mean.data + sd.data
lo.m.sd <- mean.data - sd.data

up.m.sd
lo.m.sd
```


boxplot to check outliers


```{r}
boxplot(data$total.foreign.sim)
```

```{r}
qs <- quantile(data$total.foreign.sim)
qs
iqr <- IQR(data$total.foreign.sim)
low.outlier <- qs[2] - (1.5 * iqr)
hi.outlier <- qs[4] + (1.5 * iqr)

low.outlier
hi.outlier
```

```{r}
data$total.foreign.sim[data$total.foreign.sim<low.outlier] = mean.data
data$total.foreign.sim[data$total.foreign.sim>hi.outlier] = mean.data
```

Checking last elements of the serie

```{r}
last5 <- data$total.foreign.sim[seq(from=length(data$total.foreign.sim)-5, to=length(data$total.foreign.sim), by=1)]
last5[last5<lo.m.sd] <- mean.data
data$total.foreign.sim[seq(from=length(data$total.foreign.sim)-5, to=length(data$total.foreign.sim), by=1)] <- last5
```

```{r}
#data$total.foreign.sim <- log(data$total.foreign.sim)
data$total.foreign.sim <- sqrt(data$total.foreign.sim)
```

## Hist after the transformation

```{r}
hist(data$total.foreign.sim)
```

## Boxplot after the transformation

```{r}
boxplot(data$total.foreign.sim)
```

# Time serie is built

Here the time serie is built

```{r}
require(xts)
data$Date <- as.Date(data$Date, format = "%Y-%m-%d")
#typeof(data$date[1])
data.xts <- xts(data$total.foreign.sim, order.by=data$Date, frequency = 7)
data.ts <- ts(data$total.foreign.sim, frequency = 7)
```

We loaded the dataset from the various datasets aggregating into only one dataset with 655 rows representing 2 years of data gathered. Starting from 05.06.2017 to 30.10.2019. Data is here:

```{r}
main <- "foreign sim per day"
ylab<-"Tot of sim in that day"
plot(data.xts,ylab=ylab,main=main)
```



```{r}
#main <- "foreign sim per day"
#ylab<-"Tot of sim in that day"
#plot(data.ts,ylab=ylab,main=main)
```


## Week by Week plot

As we can see the greatest number of foreign come after the mid of the week

```{r}
boxplot(data.ts~cycle(data.ts))
```

# Peaks Explanation

Many peaks are present we would like to exaplin them and to cut them out to be able to predict with a simple arima

- automatic roaming [https://www.mobileworld.it/2017/08/07/roaming-gratis-europa-condizioni-fair-use-114077/]
- fashion week [https://www.cameramoda.it/it/milano-moda-donna/] february
- fashion week 2017 [https://www.milanoweekend.it/articoli/milano-fashion-week-2017-eventi-programma/] february

```{r}
a <- max(data$total.foreign.sim)
b <- data$Date[data$total.foreign.sim==a]
```

- arch week [https://www.lastampa.it/milano/2017/06/17/news/milano-smart-city-del-futuro-se-ne-parla-all-archweek-in-triennale-1.34584894?refresh_ce]
- it was a saturday!!
- it was the orient festival [https://www.wikieventi.it/milano/index.php?data_selezionata=2017-06-17]
- many mucis events, samsara of papetee and others, folk's festivals, discounts [https://www.wikieventi.it/milano/index.php?data_selezionata=2017-07-22]

# Trend recognition

```{r}
tt<-as.numeric(time(data.ts))
fit2<-lm(data.ts~poly(tt,degree=2,raw=TRUE))
fit4<-lm(data.ts~poly(tt,degree=8,raw=TRUE))

main <- "foreign sim per day"
plot(data.ts,ylab=ylab,main=main)
lines(tt,predict(fit2),col='red',lwd=2)
lines(tt,predict(fit4),col='blue',lwd=2)
legend("bottomright",legend = c("2nd order","4th order"),lwd=2,lty=1,col=c("red","blue"))
```

## Smoothing

```{r}
require(fpp)
require(forecast)
trendpattern = timeSeries::filter(data.ts, filter = rep(1/15,15), sides=2)
plot(data.ts, main = "moving average annual trend")
lines(trendpattern,col="red")
```

```{r}
seasonality <- data.ts - trendpattern
plot(seasonality)
```

## Splitting

```{r}
require(TSstudio)
split <- ts_split(ts.obj = data.ts, sample.out = 100)

data.ts <- split$train
testing <- split$test
```

# Models

We tried many models, the most known is the arma but we tried:
- arma
- arima
- sarima
- var
- rugarch
- fgarch

and others, i'm reporting here only the best one and some ideas for the worst performing ones.

## Arima

To find the best arima in a such complicate time serie we are going to exploit a grid search algorithm done via the auto arima function provided by the fpp package

```{r}
require(fpp)
require(forecast)
fit <- auto.arima(data.ts,stepwise=TRUE,trace=TRUE,ic = "bic", approximation = FALSE)
```

A forecast on the training set looks like this one below:

```{r}
plot(forecast(fit))
```

Here we are going to plot a forecast on a part of the data that was never seen by the arima model, the plots looks not so bad.

```{r}
#training <- subset(auscafe, end=length(auscafe)-61)
#test <- subset(auscafe, start=length(auscafe)-60)
#cafe.train <- Arima(training, order=c(2,1,1),seasonal=c(0,1,2), lambda=0)
data.ts %>% forecast(h=100) %>% autoplot() + autolayer(testing)
```

The accuracy on the test set is only two percentages points lower than on the training set. We are using 100 points.

```{r}
pred <- forecast(fit, h=100)
accuracy(pred, testing)
```

Actually here below we can see that residuals are not pretty good but after many weeks of attempts with many models, all the poossible seasonalities, all the decomposition, all the tricks available i'm pretty sure that this is the best trade-off between complexity of the models used, accuracy and processing of the dataset.

```{r}
tsdisplay(residuals(fit))
```

```{r}
lb <- Box.test(residuals(fit), lag = 24, type = "Ljung-Box")
lb
```

```{r}
bp <- Box.test(residuals(fit), lag = 24, type = "Box-Pierce")
bp
```

The diagnostic here below confirms at least that the auto arima chose the right differentiation parameter.

```{r}
########################################################
#### Residuals diagnostics in forecasting
########################################################

res.fr <- residuals(pred)

par(mfrow=c(1,3))

plot(res.fr, main="Residuals from ARIMA method",
  ylab="", xlab="Years")

Acf(res.fr, main="ACF of residuals")

u <- residuals(fit)

m<-mean(u)
std<-sqrt(var(u))
hist(u, breaks=20, col="gray", prob=TRUE, 
xlab="Residuals", main="Histogram of residuals\n with Normal Curve")
curve(dnorm(x, mean=m, sd=std), 
      col="black", lwd=2, add=TRUE)
```

```{r}
library(tseries)
kpss.test(diff(data.ts))
ndiffs(data.ts)
plot(data.ts)
```


The plot is not good but AIC and BIC are very high, we should try with a multi seasonal decomposition

```{r}
frequency(data.ts)
```

## Searching for multi seasonalities

We searched for all the possible multi seasonalities using the msts package but at the end adding many seasonalities did not helped in finding better residuals or clearer trends and seasonalities. Adding more and more just created a complicated useless model that scored the same as the base one.

```{r}
# library(lubridate)
# library(dplyr)
# library(forecast)
# library(ggplot2)
# library(scales)
# 
# data.ts %>% decompose(type="multiplicative") %>% autoplot()
# dec <- decompose(data.ts)
# acf(na.omit(dec$random), lag.max = 30)
```

## RUGARCH

We tried this model but it was not useful for great predictions, residuals were much more scattered than arima.

```{r}
# require(rugarch)
# spec = ugarchspec()
# fit = ugarchfit(spec = spec, data = data.ts)
# show(fit)
```


```{r}
# plot(fit, which='all')
```



# Conclusions


It was a hard work! Iwas really interesting! I tried a lot of methods to fit the data and obtain good forecasting and good residuals. I tried searching for multi seasonalities, difference some times to reach stationarity, detrending with lm and ma, smoothing. I tried by hand many arima models. I tried to decompose the time serie in many ways. I tried all the possible frequencies that can be thought as valid. Eventually it was really interesting! I experimented a lot.

The lesson I learned from this dataset is that the most foreign come after the mid of the week and the peak of foreign is unexpectedly on Friday and secondarily on Saturday. The number of outliers is very high due to programmed events that break the seasonality of the time serie. The number of foreign connected to the openwifi surely can be a good proxy for the overall number of foreign in milan since the antennas are in the central part of the city. In the days after the break down of the roaming policy the number of foreign surely increased but we cannot prove this fact since the data we have is not enough.

